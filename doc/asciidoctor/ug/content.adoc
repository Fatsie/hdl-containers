[IMPORTANT]
====
Image names and tags in this documentation are provided without the _registry prefix_.
Hence, `REGISTRY/[ARCHITECTURE/][COLLECTION/]*` needs to be prefixed to the image names shown in link:../index.html#_tools_and_images[Tools and images].
====

Official guidelines and recommendations for using containers suggest keeping them small and specific for each tool/purpose (see https://docs.docker.com/develop/develop-images/dockerfile_best-practices/[docs.docker.com: Best practices for writing Dockerfiles]). That fits well with the field of web _microservices_, which communicate through TCP/IP and which need to be composed, scaled and balanced all around the globe.

However, tooling in other camps is expected to communicate using a shared or local filesystem and/or pipes; therefore, many users treat containters as _lightweight virtual machines_. That is, they put all the tools in a single (heavy) container. Those containers are typically not moved around as frequently as _microservices_, but cached on developers' workstations.

In this project, both paradigms are supported; fine-grained images are available, as well as all-in-one images.

[NOTE]
====
`hdl` examples in https://github.com/im-tomu/fomu-workshop[im-tomu/fomu-workshop] showcase a Makefile based
solution that supports both strategies: the fine-grained pulling and the all-in-one approach.
An environment variable (`CONTAINER_ENGINE`) is used for selecting which approach to use.
For didactic purposes, both of them are used in Continuous Integration (CI).
See https://github.com/im-tomu/fomu-workshop/blob/master/.github/workflows/test.yml[im-tomu/fomu-workshop: .github/workflows/test.yml].
====


== Fine-grained pulling

NOTE: These images are coloured [lime]#GREEN# in the link:../dev/index.html#_graphs[Graphs].

Ready-to-use images are provided for each tool, which contain the tool and the dependencies for it to run successfully. These are typically named `REGISTRY_PREFIX/[ARCHITECTURE/][COLLECTION/]TOOL_NAME`.

NOTE: Since all the images in each collection are based on the same root image, pulling multiple images involves
  retrieving a few additional layers only. Therefore, this is the recommended approach for CI or other environments with
  limited resources.

* https://github.com/antonblanchard/ghdl-yosys-blink/blob/master/Makefile[ghdl-yosys-blink: Makefile]: an example
  showcasing how to use this fine-grained approach with a makefile.
  The same make based strategy is used in https://github.com/antonblanchard/microwatt/blob/master/Makefile[antonblanchard/microwatt].
* https://github.com/marph91/icestick-remote[marph91/icestick-remote]: the CI workflow for synthesis uses this approach.

Those projects use a partial Makefile such as the following, for optionally wrapping regular tool calls:

[bash]
----
CONTAINER_ENGINE ?= docker

PWD = $(shell pwd)
CONTAINER_ARGS = run --rm -v $(PWD):/wrk -w /wrk

GHDL    = $(CONTAINER_ENGINE) $(CONTAINER_ARGS) gcr.io/hdl-containers/ghdl/yosys ghdl
YOSYS   = $(CONTAINER_ENGINE) $(CONTAINER_ARGS) gcr.io/hdl-containers/ghdl/yosys yosys
NEXTPNR = $(CONTAINER_ENGINE) $(CONTAINER_ARGS) gcr.io/hdl-containers/nextpnr/ice40 nextpnr-ice40
ICEPACK = $(CONTAINER_ENGINE) $(CONTAINER_ARGS) gcr.io/hdl-containers/icestorm icepack
----

Moreover, https://github.com/PyFPGA/[PyFPGA] is a set of Python classes for vendor-independent FPGA development.
https://github.com/PyFPGA/openflow[PyFPGA/openflow] allows running GHDL, Yosys, etc. in containers.
In fact, openflow can be used along with https://github.com/olofk/edalize[Edalize]'s `EDALIZE_LAUNCHER` environment
variable.
See also https://github.com/librecores/eda-container-wrapper[librecores/eda-container-wrapper].


== All-in-one images

NOTE: These images are coloured [maroon]#BROWN# in the link:../dev/index.html#_graphs[Graphs].

Multiple tools from fine-grained images are included in larger images for common use cases.
These are named `REGISTRY_PREFIX/[ARCHITECTURE/][COLLECTION/]MAIN_USAGE`.
This is the recommended approach for users who are less familiar with containers and want a quick replacement for
full-featured virtual machines.
Coherently, some common Unix tools (such as make or cmake) are also included in these all-in-one images.

* https://github.com/tmeissner/formal_hw_verification[tmeissner/formal_hw_verification]: the CI workflow uses image `REGISTRY_PREFIX/[ARCHITECTURE/][COLLECTION/]formal/all` along with GitHub's 'Docker Action' syntax (see https://docs.github.com/en/free-pro-team@latest/actions/learn-github-actions/finding-and-customizing-actions#referencing-a-container-on-docker-hub[docs.github.com: Learn GitHub Actions > Referencing a container on Docker Hub]).
* https://github.com/stnolting/neorv32[stnolting/neorv32]: the implementation workflow (for generating bitstreams from VHDL sources) uses image `REGISTRY_PREFIX/[ARCHITECTURE/][COLLECTION/]impl` along with GitHub's 'Docker Action' syntax (see https://docs.github.com/en/free-pro-team@latest/actions/learn-github-actions/finding-and-customizing-actions#referencing-a-container-on-docker-hub[docs.github.com: Learn GitHub Actions > Referencing a container on Docker Hub]).

=== SymbiFlow

As explained in link:../dev/index.html#_symbiflow_conda[SymbiFlow (Conda)], multiple ready-to-use images are provided
including Miniconda, SymbiFlow toolchains and architecture definitions for Xilinx's __xc7__ or QuickLogic's __eos-s3__
devices.
These container images are expected to be used as explained in https://symbiflow-examples.rtfd.io/en/latest/building-examples.html[image:https://img.shields.io/website.svg?label=symbiflow-examples.rtfd.io&longCache=true&style=flat-square&url=http%3A%2F%2Fsymbiflow-examples.rtfd.io%2Fen%2Flatest%2Findex.html&logo=ReadTheDocs&logoColor=fff[title='symbiflow-examples.rtfd.io']], assuming that the environment is prepared already and available in the PATH.
Hence, the Conda environment can be activated straightaway.
See, for instance:

[source, bash]
----
:~# git clone https://github.com/SymbiFlow/symbiflow-examples
...
:~# cd symbiflow-examples

:~/symbiflow-examples# docker run --rm -it \
  -v /$(pwd)://wrk \
  -w //wrk \
  gcr.io/hdl-containers/conda/symbiflow/xc7/a100t
...
(xc7) root@c3d4dd1d97cc:/wrk# TARGET="arty_100" make -C xc7/picosoc_demo/
...
(xc7) root@c3d4dd1d97cc:/wrk# ls -1 xc7/picosoc_demo/build/arty_100/
constraints.place
fasm.log
pack.log
packing_pin_util.rpt
place.log
pre_pack.report_timing.setup.rpt
report_timing.hold.rpt
report_timing.setup.rpt
report_unconstrained_timing.hold.rpt
report_unconstrained_timing.setup.rpt
route.log
top.bit
top.eblif
top.fasm
top.ioplace
top.json
top.json.carry_fixup.json
top.json.carry_fixup_out.json
top.json.post_abc9.ilang
top.json.pre_abc9.ilang
top.net
top.net.post_routing
top.place
top.route
top.sdc
top_io.json
top_synth.log
top_synth.v
top_synth.v.premap.v
----

== Tools with GUI

By default, tools with Graphical User Interface (GUI) cannot be used in containers, because there is no graphical
server.
However, there are multiple alternatives for making an https://en.wikipedia.org/wiki/X_Window_System[X11] or
https://en.wikipedia.org/wiki/Wayland_(display_server_protocol)[Wayland] server visible to the container.
https://github.com/mviereck/x11docker[mviereck/x11docker] and https://github.com/mviereck/runx[mviereck/runx] are
full-featured helper scripts for setting up the environment and running GUI applications and desktop environments in OCI
containers.
GNU/Linux and Windows hosts are supported, and security related options are provided (such as cookie authentication).
Users of GTKWave, KLayout, nextpnr and other tools will likely want to try x11docker (and runx).

* https://joss.theoj.org/papers/10.21105/joss.01349[x11docker: Run GUI applications in Docker containers; Journal of Open Source Hardware].

[#img-x11docker]
.Execution of KLayout in a container on Windows 10 (MSYS2/MINGW64) with https://github.com/mviereck/x11docker[mviereck/x11docker], https://github.com/mviereck/runx[mviereck/runx] and https://sourceforge.net/projects/vcxsrv/[VcxSrv].
[link=img/x11docker_klayout.gif]
image::x11docker_klayout.gif[x11docker_klayout, align="center"]

== USB/IP protocol support for Docker Desktop

Virtual Machines used on Windows for running either Windows Subsystem for Linux (WSL) or Docker Desktop by default do
not support sharing USB devices with the containers.
Only those that are identified as storage or COM devices can be bind directly.
See https://github.com/microsoft/WSL/issues/5158[microsoft/WSL#5158].
That prevents using arbitrary drivers inside the containers.
As a result, most container users on Windows do install board programming tools through MSYS2 (see https://github.com/hdl/MINGW-packages[hdl/MINGW-packages]).

Nevertheless, USB/IP protocol allows passing USB device(s) from server(s) to client(s) over the network.
As explained at https://www.kernel.org/doc/readme/tools-usb-usbip-README[kernel.org/doc/readme/tools-usb-usbip-README],
on GNU/Linux, USB/IP is implemented as a few kernel modules with companion userspace tools.
However, the default underlying Hyper-V VM machine (based on https://alpinelinux.org/[Alpine Linux]) shipped with
_Docker Desktop_ (aka _docker-for-win_/_docker-for-mac_) does not include the required kernel modules.
Fortunately, privileged docker containers allow installing missing kernel modules.
The shell script in link:{repotree}usbip/[`usbip/`] supports customising the native VM in _Docker Desktop_ for adding
USB over IP support.

[source, bash]
----
# Build kernel modules: in an unprivileged `alpine` container, retrieve the corresponding
# kernel sources, copy runtime config and enable USB/IP features, build `drivers/usb/usbip`
# and save `*.ko` artifacts to relative subdir `dist` on the host.
./run.sh -m

# Load/insert kernel modules: use a privileged `busybox` container to load kernel modules
# `usbip-core.ko` and `vhci-hcd.ko` from relative subdir `dist` on the host to the
# underlying Hyper-V VM.
./run.sh -l

# Build image `vhcli`, using `busybox` as a base, and including the
# [VirtualHere](https://www.virtualhere.com) GNU/Linux client for x86_64 along with the
# `*.ko` files built previously through `./run.sh -m`.
./run.sh -v
----

NOTE: For manually selecting configuration options, building and inserting modules, see detailed procedure in https://github.com/gw0/docker-alpine-kernel-modules#usage[gw0/docker-alpine-kernel-modules#usage].

NOTE: Modules will be removed when the Hyper-V VM is restarted (i.e. when the host or _Docker Desktop_ are restarted). For a _permanent_ install, modules need to be copied to `/lib/modules` in the underlying VM, and `/stc/modules` needs to be configured accordingly. Use `$(command -v winpty) docker run --rm -it --privileged --pid=host alpine nsenter -t 1 -m -u -n -i sh` to access a shell with full permissions on the VM.

NOTE: USB/IP is supported in Renode too. See https://renode.readthedocs.io/en/latest/tutorials/usbip.html[renode.rtfd.io/en/latest/tutorials/usbip].

=== Example session

How to connect a _Docker Desktop_ container to _VirtualHere USB Server for Windows_.

* Start https://www.virtualhere.com/sites/default/files/usbserver/vhusbdwin64.exe[`vhusbdwin64.exe`] on the host
* Ensure that the firewall is not blocking it.

[source, bash]
----
# Start container named 'vhclient'
./run.sh -s
# List usb devices available in the container
./run.sh -e lsusb
# LIST hubs/devices found by vhclient
./run.sh -c "LIST"
# Manually add to the client the hub/server running on the host
./run.sh -c "MANUAL HUB ADD,host.docker.internal:7575"

sleep 10

./run.sh -c "LIST"
# Use a remote device in the container
./run.sh -c "USE,<SERVER HOSTNAME>.1"

sleep 4

# Check that the device is now available in the container
./run.sh -e lsusb
----

IMPORTANT: There is an issue/bug in _Docker Desktop_ (https://github.com/docker/for-win/issues/4548[docker/for-win#4548]) that prevents the container where the USB device is added from seeing it. The workaround is to execute the board programming tool in a sibling container. For example: `docker run --rm --privileged */prog iceprog -t`.

=== Alternatives

[IMPORTANT]
====
Using https://www.virtualhere.com[VirtualHere] is the only solution we could successfully use in order to share FTDI devices (https://www.latticesemi.com/icestick[icestick] boards) between a Windows 10 host and a Docker Desktop container running on the same host. However, since the USB/IP protocol is open source, we'd like to try any other (preferredly open and free source) server for Windows along with the default GNU/Linux usbip-tools. Should you know about any, please https://github.com/hdl/containers/issues/new[let us know]!

We are aware of https://github.com/cezuni/usbip-win[cezuni/usbip-win]. However, it seems to be in very early development state and the install procedure is quite complex yet.
====

Serial (COM) devices can be shared with open source tools. On the one hand, https://sourceforge.net/projects/com0com/files/hub4com/[hub4com] from project http://com0com.sourceforge.net/[com0com] allows to publish a port through a RFC2217 server. On the other hand, `socat` can be used to link the network connection to a virtual `tty` device.

[source]
----
                   HOST                                           CONTAINER
        ---------------------------                 -------------------------------------
USB <-> | COMX <-> RFC2217 server | <-> network <-> | socat <-> /dev/ttySY <-> app/tool |
        ---------------------------                 -------------------------------------
----

[source, cmd]
----
REM On the Windows host
com2tcp-rfc2217.bat COM<X> <PORT>
----

[source, bash]
----
# In the container
socat pty,link=/dev/ttyS<Y> tcp:host.docker.internal:<PORT>
----

It might be possible to replace `hub4com` with https://github.com/pyserial/pyserial[pyserial/pyserial]. However, we did not test it.

* https://pyserial.readthedocs.io/en/latest/examples.html#single-port-tcp-ip-serial-bridge-rfc-2217
* https://github.com/espressif/esp-idf/issues/204[espressif/esp-idf#204]
