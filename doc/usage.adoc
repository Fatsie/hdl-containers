== Usage

Official guidelines and recommendations for using containers suggest keeping containers small and specific for each tool/purpose. That fits well with the field of web _microservices_, which communicate through TCP/IP and which need to be composed, scaled and balanced all around the globe.

However, tooling in other camps is expected to communicate using a shared or local filesystem and/or pipes; therefore, many users treat containters as _lightweight virtual machines_. That is, they put all the tools in a single (heavy) container. Those containers are typically not moved around as frequently as _microservices_, but cached on developers' workstations.

In this project, both paradigms are supported; fine-grained images are available, as well as all-in-one images.


=== Fine-grained pulling

Ready-to-use images are provided for each tool, which contain the tool and the dependencies for it to run successfully. These are typically named `hdlc/<TOOL_NAME>`. Since all of them are based on the same root image, pulling multiple images involves retrieving a few additional layers only. Therefore, this is the recommended approach for CI or other environments with limited resources.

* There is an example at https://github.com/antonblanchard/ghdl-yosys-blink/blob/master/Makefile[ghdl-yosys-blink: Makefile] showcasing how to use this fine-grained approach with a makefile.
* At https://github.com/marph91/icestick-remote[marph91/icestick-remote] the CI workflow for synthesis uses this approach.
* Moreover, https://gitlab.com/rodrigomelo9/pyfpga[PyFPGA] is a Python Class for vendor-independent FPGA development, which runs GHDL, Yosys, etc. in containers.

NOTE: These images are coloured [lime]#GREEN# in the <<Graph>>.


=== All-in-one images

Multiple tools from fine-grained images are included in larger images for common use cases. These are named `hdlc/<MAIN_USAGE>`. This is the recommended approach for users who are less familiar with containers and want a quick replacement for full-featured virtual machines. Coherently, some common Unix tools (such as make or cmake) are also included in these all-in-one imags.

NOTE: These images are coloured [maroon]#BROWN# in the <<Graph>>.


=== USB/IP protocol support for Docker Desktop

USB/IP protocol allows to pass USB device(s) from server(s) to client(s) over the network. As explained at https://www.kernel.org/doc/readme/tools-usb-usbip-README[kernel.org/doc/readme/tools-usb-usbip-README], on GNU/Linux, USB/IP is implemented as a few kernel modules with companion userspace tools. However, the default underlying Hyper-V VM machine (based on https://alpinelinux.org/[Alpine Linux]) shipped with _Docker Desktop_ (aka _docker-for-win_/_docker-for-mac_) does not include the required kernel modules. Fortunately, privileged docker containers allow to install missing kernel modules. The shell script in link:{repotree}usbip/[`usbip/`] supports customising the native VM in _Docker Desktop_ for adding USB over IP support.

[source, bash]
----
# Build kernel modules: in an unprivileged `alpine` container, retrieve the corresponding
# kernel sources, copy runtime config and enable USB/IP features, build `drivers/usb/usbip`
# and save `*.ko` artifacts to relative subdir `dist` on the host.
./run.sh -m

# Load/insert kernel modules: use a privileged `busybox` container to load kernel modules
# `usbip-core.ko` and `vhci-hcd.ko` from relative subdir `dist` on the host to the
# underlying Hyper-V VM.
./run.sh -l

# Build image `vhcli`, using `busybox` as a base, and including the
# [VirtualHere](https://www.virtualhere.com) GNU/Linux client for x86_64 along with the
# `*.ko` files built previously through `./run.sh -m`.
./run.sh -v
----

NOTE: For manually selecting configuration options, building and inserting modules, see detailed procedure in https://github.com/gw0/docker-alpine-kernel-modules#usage[gw0/docker-alpine-kernel-modules#usage].

NOTE: Modules will be removed when the Hyper-V VM is restarted (i.e. when the host or _Docker Desktop_ are restarted). For a _permanent_ install, modules need to be copied to `/lib/modules` in the underlying VM, and `/stc/modules` needs to be configured accordingly. Use `$(command -v winpty) docker run --rm -it --privileged --pid=host alpine nsenter -t 1 -m -u -n -i sh` to access a shell with full permissions on the VM.

==== Example session

How to connect a _Docker Desktop_ container to _VirtualHere USB Server for Windows_.

* Start https://www.virtualhere.com/sites/default/files/usbserver/vhusbdwin64.exe[`vhusbdwin64.exe`] on the host
* Ensure that the firewall is not blocking it.

[source, bash]
----
# Start container named 'vhclient'
./run.sh -s
# List usb devices available in the container
./run.sh -e lsusb
# LIST hubs/devices found by vhclient
./run.sh -c "LIST"
# Manually add to the client the hub/server running on the host
./run.sh -c "MANUAL HUB ADD,host.docker.internal:7575"

sleep 10

./run.sh -c "LIST"
# Use a remote device in the container
./run.sh -c "USE,<SERVER HOSTNAME>.1"

sleep 4

# Check that the device is now available in the container
./run.sh -e lsusb
----

IMPORTANT: There is an issue/bug in _Docker Desktop_ (https://github.com/docker/for-win/issues/4548[docker/for-win#4548]) that prevents the container where the USB device is added from seeing it. The workaround is to execute the board programming tool in a sibling container. For example: `docker run --rm --privileged hdlc/prog iceprog -t`.

==== Alternatives

[IMPORTANT]
====
Using https://www.virtualhere.com[VirtualHere] is the only solution we could successfully use in order to share FTDI devices (https://www.latticesemi.com/icestick[icestick] boards) between a Windows 10 host and a Docker Desktop container running on the same host. However, since the USB/IP protocol is open source, we'd like to try any other (preferredly open and free source) server for Windows along with the default GNU/Linux usbip-tools. Should you know about any, please https://github.com/hdl/containers/issues/new[let us know]!

We are aware of https://github.com/cezuni/usbip-win[cezuni/usbip-win]. However, it seems to be in very early development state and the install procedure is quite complex yet.
====

Serial (COM) devices can be shared with open source tools. On the one hand, https://sourceforge.net/projects/com0com/files/hub4com/[hub4com] from project http://com0com.sourceforge.net/[com0com] allows to publish a port through a RFC2217 server. On the other hand, `socat` can be used to link the network connection to a virtual `tty` device.

[source]
----
                   HOST                                           CONTAINER
        ---------------------------                 -------------------------------------
USB <-> | COMX <-> RFC2217 server | <-> network <-> | socat <-> /dev/ttySY <-> app/tool |
        ---------------------------                 -------------------------------------
----

[source, cmd]
----
REM On the Windows host
com2tcp-rfc2217.bat COM<X> <PORT>
----

[source, bash]
----
# In the container
socat pty,link=/dev/ttyS<Y> tcp:host.docker.internal:<PORT>
----

It might be possible to replace `hub4com` with https://github.com/pyserial/pyserial[pyserial/pyserial]. However, we did not test it.

* https://pyserial.readthedocs.io/en/latest/examples.html#single-port-tcp-ip-serial-bridge-rfc-2217
* https://github.com/espressif/esp-idf/issues/204[espressif/esp-idf#204]
