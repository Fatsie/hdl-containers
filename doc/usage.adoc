== Usage

IMPORTANT: Image names and tags in this documentation are provided without the _registry prefix_. Hence, `ghcr.io/hdl/debian-buster/*`, `ghcr.io/hdl/centos-7/*` or `hdlc/*` needs to be prefixed to the image names shown in <<Tools and images>>.

Official guidelines and recommendations for using containers suggest keeping containers small and specific for each tool/purpose (see https://docs.docker.com/develop/develop-images/dockerfile_best-practices/[docs.docker.com: Best practices for writing Dockerfiles]). That fits well with the field of web _microservices_, which communicate through TCP/IP and which need to be composed, scaled and balanced all around the globe.

However, tooling in other camps is expected to communicate using a shared or local filesystem and/or pipes; therefore, many users treat containters as _lightweight virtual machines_. That is, they put all the tools in a single (heavy) container. Those containers are typically not moved around as frequently as _microservices_, but cached on developers' workstations.

In this project, both paradigms are supported; fine-grained images are available, as well as all-in-one images.


=== Fine-grained pulling

Ready-to-use images are provided for each tool, which contain the tool and the dependencies for it to run successfully. These are typically named `<REGISTRY_PREFIX>/<TOOL_NAME>`. Since all the images in each collection are based on the same root image, pulling multiple images involves retrieving a few additional layers only. Therefore, this is the recommended approach for CI or other environments with limited resources.

* There is an example at https://github.com/antonblanchard/ghdl-yosys-blink/blob/master/Makefile[ghdl-yosys-blink: Makefile] showcasing how to use this fine-grained approach with a makefile. The same make based strategy is used in https://github.com/antonblanchard/microwatt/blob/master/Makefile[antonblanchard/microwatt].
* At https://github.com/marph91/icestick-remote[marph91/icestick-remote] the CI workflow for synthesis uses this approach.
* Moreover, https://github.com/PyFPGA/[PyFPGA] is a set of Python classes for vendor-independent FPGA development. https://github.com/PyFPGA/openflow[PyFPGA/openflow] allows running GHDL, Yosys, etc. in containers.

NOTE: These images are coloured [lime]#GREEN# in the <<Graphs>>.


=== All-in-one images

Multiple tools from fine-grained images are included in larger images for common use cases. These are named `<REGISTRY_PREFIX>/<MAIN_USAGE>`. This is the recommended approach for users who are less familiar with containers and want a quick replacement for full-featured virtual machines. Coherently, some common Unix tools (such as make or cmake) are also included in these all-in-one imags.

* The CI workflow in https://github.com/tmeissner/formal_hw_verification[tmeissner/formal_hw_verification] uses image `<REGISTRY_PREFIX>/formal:all` along with GitHub's 'Docker Action' syntax (see https://docs.github.com/en/free-pro-team@latest/actions/learn-github-actions/finding-and-customizing-actions#referencing-a-container-on-docker-hub[docs.github.com: Learn GitHub Actions > Referencing a container on Docker Hub]).

NOTE: These images are coloured [maroon]#BROWN# in the <<Graphs>>.


=== Tools with GUI

By default, tools with Graphical User Interface (GUI) cannot be used in containers, because there is no graphical server. However, there are multiple alternatives for making an https://en.wikipedia.org/wiki/X_Window_System[X11] or https://en.wikipedia.org/wiki/Wayland_(display_server_protocol)[Wayland] server visible to the container. https://github.com/mviereck/x11docker[mviereck/x11docker] and https://github.com/mviereck/runx[mviereck/runx] are full-featured helper scripts for setting up the environment and running GUI applications and desktop environments in OCI containers. GNU/Linux and Windows hosts are supported, and security related options are provided (such as cookie authentication). Users of GTKWave, KLayout, nextpnr and other tools will likely want to try x11docker (and runx).

* https://joss.theoj.org/papers/10.21105/joss.01349[x11docker: Run GUI applications in Docker containers; Journal of Open Source Hardware].

[#img-x11docker]
.Execution of KLayout in a container on Windows 10 (MSYS2/MINGW64) with https://github.com/mviereck/x11docker[mviereck/x11docker], https://github.com/mviereck/runx[mviereck/runx] and https://sourceforge.net/projects/vcxsrv/[VcxSrv].
[link=img/x11docker_klayout.gif]
image::img/x11docker_klayout.gif[x11docker_klayout, align="center"]

=== USB/IP protocol support for Docker Desktop

USB/IP protocol allows to pass USB device(s) from server(s) to client(s) over the network. As explained at https://www.kernel.org/doc/readme/tools-usb-usbip-README[kernel.org/doc/readme/tools-usb-usbip-README], on GNU/Linux, USB/IP is implemented as a few kernel modules with companion userspace tools. However, the default underlying Hyper-V VM machine (based on https://alpinelinux.org/[Alpine Linux]) shipped with _Docker Desktop_ (aka _docker-for-win_/_docker-for-mac_) does not include the required kernel modules. Fortunately, privileged docker containers allow to install missing kernel modules. The shell script in link:{repotree}usbip/[`usbip/`] supports customising the native VM in _Docker Desktop_ for adding USB over IP support.

[source, bash]
----
# Build kernel modules: in an unprivileged `alpine` container, retrieve the corresponding
# kernel sources, copy runtime config and enable USB/IP features, build `drivers/usb/usbip`
# and save `*.ko` artifacts to relative subdir `dist` on the host.
./run.sh -m

# Load/insert kernel modules: use a privileged `busybox` container to load kernel modules
# `usbip-core.ko` and `vhci-hcd.ko` from relative subdir `dist` on the host to the
# underlying Hyper-V VM.
./run.sh -l

# Build image `vhcli`, using `busybox` as a base, and including the
# [VirtualHere](https://www.virtualhere.com) GNU/Linux client for x86_64 along with the
# `*.ko` files built previously through `./run.sh -m`.
./run.sh -v
----

NOTE: For manually selecting configuration options, building and inserting modules, see detailed procedure in https://github.com/gw0/docker-alpine-kernel-modules#usage[gw0/docker-alpine-kernel-modules#usage].

NOTE: Modules will be removed when the Hyper-V VM is restarted (i.e. when the host or _Docker Desktop_ are restarted). For a _permanent_ install, modules need to be copied to `/lib/modules` in the underlying VM, and `/stc/modules` needs to be configured accordingly. Use `$(command -v winpty) docker run --rm -it --privileged --pid=host alpine nsenter -t 1 -m -u -n -i sh` to access a shell with full permissions on the VM.

==== Example session

How to connect a _Docker Desktop_ container to _VirtualHere USB Server for Windows_.

* Start https://www.virtualhere.com/sites/default/files/usbserver/vhusbdwin64.exe[`vhusbdwin64.exe`] on the host
* Ensure that the firewall is not blocking it.

[source, bash]
----
# Start container named 'vhclient'
./run.sh -s
# List usb devices available in the container
./run.sh -e lsusb
# LIST hubs/devices found by vhclient
./run.sh -c "LIST"
# Manually add to the client the hub/server running on the host
./run.sh -c "MANUAL HUB ADD,host.docker.internal:7575"

sleep 10

./run.sh -c "LIST"
# Use a remote device in the container
./run.sh -c "USE,<SERVER HOSTNAME>.1"

sleep 4

# Check that the device is now available in the container
./run.sh -e lsusb
----

IMPORTANT: There is an issue/bug in _Docker Desktop_ (https://github.com/docker/for-win/issues/4548[docker/for-win#4548]) that prevents the container where the USB device is added from seeing it. The workaround is to execute the board programming tool in a sibling container. For example: `docker run --rm --privileged */prog iceprog -t`.

==== Alternatives

[IMPORTANT]
====
Using https://www.virtualhere.com[VirtualHere] is the only solution we could successfully use in order to share FTDI devices (https://www.latticesemi.com/icestick[icestick] boards) between a Windows 10 host and a Docker Desktop container running on the same host. However, since the USB/IP protocol is open source, we'd like to try any other (preferredly open and free source) server for Windows along with the default GNU/Linux usbip-tools. Should you know about any, please https://github.com/hdl/containers/issues/new[let us know]!

We are aware of https://github.com/cezuni/usbip-win[cezuni/usbip-win]. However, it seems to be in very early development state and the install procedure is quite complex yet.
====

Serial (COM) devices can be shared with open source tools. On the one hand, https://sourceforge.net/projects/com0com/files/hub4com/[hub4com] from project http://com0com.sourceforge.net/[com0com] allows to publish a port through a RFC2217 server. On the other hand, `socat` can be used to link the network connection to a virtual `tty` device.

[source]
----
                   HOST                                           CONTAINER
        ---------------------------                 -------------------------------------
USB <-> | COMX <-> RFC2217 server | <-> network <-> | socat <-> /dev/ttySY <-> app/tool |
        ---------------------------                 -------------------------------------
----

[source, cmd]
----
REM On the Windows host
com2tcp-rfc2217.bat COM<X> <PORT>
----

[source, bash]
----
# In the container
socat pty,link=/dev/ttyS<Y> tcp:host.docker.internal:<PORT>
----

It might be possible to replace `hub4com` with https://github.com/pyserial/pyserial[pyserial/pyserial]. However, we did not test it.

* https://pyserial.readthedocs.io/en/latest/examples.html#single-port-tcp-ip-serial-bridge-rfc-2217
* https://github.com/espressif/esp-idf/issues/204[espressif/esp-idf#204]
